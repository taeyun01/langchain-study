{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='죄송합니다, 제가 그 정보를 알 수 없습니다.'\n"
     ]
    }
   ],
   "source": [
    "# LCEL을 이용해서 Map Reduce Chain 직접 구현\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# ChatGPT 모델 초기화 - 온도값 0.1로 설정하여 일관된 응답 생성\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "# 임베딩 캐시를 저장할 로컬 디렉토리 설정\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "# 문서 분할기 설정 - tiktoken 토크나이저 사용\n",
    "# 각 청크는 600자, 100자 오버랩으로 분할\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "# test.txt 파일 로더 초기화\n",
    "loader = UnstructuredFileLoader(\"./files/test.txt\")\n",
    "\n",
    "# 문서를 청크로 분할\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# OpenAI 임베딩 모델 초기화\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 임베딩 결과를 로컬에 캐시하도록 설정\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "# FAISS 벡터 저장소 생성 - 문서와 임베딩 결과 저장\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "# 벡터 저장소를 검색기로 변환\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# 각 문서 청크를 처리할 프롬프트 템플릿 정의\n",
    "map_doc_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim. If there is no relevant text, return : ''\n",
    "            -------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 프롬프트 템플릿과 LLM을 연결하여 각 문서 청크를 처리할 체인 생성\n",
    "# map_doc_prompt: 문서 청크에서 관련 내용을 추출하는 프롬프트\n",
    "# llm: ChatGPT 모델\n",
    "# | 연산자로 프롬프트와 LLM을 연결하여 하나의 실행 가능한 체인으로 만듦\n",
    "map_doc_chain = map_doc_prompt | llm\n",
    "\n",
    "# 각 문서에서 질문과 관련된 내용을 추출하여 하나의 문자열로 합치는 함수\n",
    "# inputs: documents(검색된 문서들), question(사용자 질문)\n",
    "# return: 각 문서에서 추출된 관련 내용들을 개행으로 구분하여 합친 문자열\n",
    "def map_docs(inputs):\n",
    "    documents = inputs[\"documents\"]\n",
    "    question = inputs[\"question\"]\n",
    "    return \"\\n\\n\".join(\n",
    "        map_doc_chain.invoke({\n",
    "            \"context\": doc.page_content, \"question\": question\n",
    "        }).content for doc in documents\n",
    "    )\n",
    "\n",
    "# 검색기와 질문을 입력으로 받아 map_docs 함수를 실행하는 체인 생성\n",
    "# documents: retriever로 검색된 관련 문서들\n",
    "# question: 사용자의 질문을 그대로 전달\n",
    "# RunnableLambda(map_docs)로 검색된 문서들에서 질문과 관련된 내용을 추출하여 하나의 문자열로 합침\n",
    "map_chain = {\n",
    "    \"documents\": retriever,\n",
    "    \"question\": RunnablePassthrough(),\n",
    "} | RunnableLambda(map_docs)\n",
    "\n",
    "# LLM에게 전달될 최종 프롬프트 내용\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    # 시스템 프롬프트 내용 : 긴 문서와 질문에서 추출된 다음 부분을 바탕으로 최종 답변을 작성하세요. 답을 모르면 모른다고 하면 됩니다. 답을 만들어내려고 하지 마세요.\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            Given the following extracted parts of a long document and a question, create a final answer. \n",
    "            If you don't know the answer, just say that you don't know. Don't try to make up an answer.\n",
    "            ------\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 최종 체인 생성\n",
    "# context: map_chain으로 검색된 문서들에서 추출한 관련 내용\n",
    "# question: 사용자의 질문을 그대로 전달\n",
    "# final_prompt와 llm을 연결하여 최종 답변을 생성하는 체인\n",
    "chain = {\"context\": map_chain, \"question\": RunnablePassthrough()} | final_prompt | llm\n",
    "\n",
    "# chain.invoke()를 사용하여 질문을 처리. \"등장인물들은 누구인가요?\"라는 질문을 체인에 전달하여 답변을 생성하고 출력\n",
    "# 1. retriever로 관련 문서 검색\n",
    "# 2. map_chain으로 문서에서 관련 내용 추출 \n",
    "# 3. final_prompt와 llm으로 최종 답변 생성\n",
    "print(chain.invoke(\"등장인물들은 누구인가요?\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
